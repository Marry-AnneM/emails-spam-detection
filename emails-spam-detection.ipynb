{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to the SMS Spam Collection datasetâ€”an invaluable resource for exploring the world of SMS spam and legitimate messages. This dataset is not just a compilation of text; it's a reflection of the ongoing efforts in SMS spam research, carefully curated from various sources to create a comprehensive collection of 5,574 messages in English.\n\nAt its core, this dataset encapsulates the distinction between \"ham\" (legitimate) and \"spam\" messages, with each message tagged accordingly. The structure is simple yet powerful, with each line presenting the label (ham or spam) in column v1 and the raw text in column v2.\n\nThe journey of this dataset is rooted in meticulous data collection efforts:\n\nA manual extraction of 425 SMS spam messages from the Grumbletext Web site, a UK forum dedicated to discussing SMS spam issues.\nA subset of 3,375 randomly chosen ham messages from the NUS SMS Corpus, originating mostly from Singaporeans and students at the National University of Singapore.\nA list of 450 SMS ham messages from Caroline Tag's PhD Thesis.\nThe incorporation of the SMS Spam Corpus v.0.1 Big, comprising 1,002 ham messages and 322 spam messages, widely used in academic research on SMS spam filtering.\nAs we embark on this exploration, the goal is clear: can we leverage this dataset to develop a prediction model capable of accurately classifying spam texts? Through analysis, statistics, and machine learning methodologies, we aim to unravel the patterns and insights hidden within this corpus, contributing to the ongoing study of SMS spam filtering.\n\nJoin us in unraveling the nuances of SMS communication and the battle against spam, using data-driven approaches to enhance our understanding and defenses in the digital messaging realm.","metadata":{}},{"cell_type":"markdown","source":"# Table of Content:\n\n1. [Data Cleaning](#sec1)\n2. [EDA](#sec2)\n3. [Text Preprocessing](#sec3)\n4. [Model Building](#sec4)\n5. [Pickle Files](#sec5)","metadata":{}},{"cell_type":"markdown","source":"I import here necessary libraries, all other libraries are imported along with code.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.68078Z","iopub.execute_input":"2024-03-16T20:39:41.681233Z","iopub.status.idle":"2024-03-16T20:39:41.686574Z","shell.execute_reply.started":"2024-03-16T20:39:41.681203Z","shell.execute_reply":"2024-03-16T20:39:41.685309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.746603Z","iopub.execute_input":"2024-03-16T20:39:41.747258Z","iopub.status.idle":"2024-03-16T20:39:41.770941Z","shell.execute_reply.started":"2024-03-16T20:39:41.747226Z","shell.execute_reply":"2024-03-16T20:39:41.770024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.772486Z","iopub.execute_input":"2024-03-16T20:39:41.772955Z","iopub.status.idle":"2024-03-16T20:39:41.78339Z","shell.execute_reply.started":"2024-03-16T20:39:41.772929Z","shell.execute_reply":"2024-03-16T20:39:41.782406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Cleaning","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.815448Z","iopub.execute_input":"2024-03-16T20:39:41.81588Z","iopub.status.idle":"2024-03-16T20:39:41.821989Z","shell.execute_reply.started":"2024-03-16T20:39:41.815847Z","shell.execute_reply":"2024-03-16T20:39:41.821198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop last 3 columns; which are not usefull\ndf = df[[\"v1\",\"v2\"]]","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.82393Z","iopub.execute_input":"2024-03-16T20:39:41.824304Z","iopub.status.idle":"2024-03-16T20:39:41.837434Z","shell.execute_reply.started":"2024-03-16T20:39:41.82427Z","shell.execute_reply":"2024-03-16T20:39:41.836081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rename columns names\ndf.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.839742Z","iopub.execute_input":"2024-03-16T20:39:41.840184Z","iopub.status.idle":"2024-03-16T20:39:41.847176Z","shell.execute_reply.started":"2024-03-16T20:39:41.840149Z","shell.execute_reply":"2024-03-16T20:39:41.845987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label encoding target column\ndf[\"target\"] = df[\"target\"].map({\"ham\":0, \"spam\":1})","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.849727Z","iopub.execute_input":"2024-03-16T20:39:41.850106Z","iopub.status.idle":"2024-03-16T20:39:41.859477Z","shell.execute_reply.started":"2024-03-16T20:39:41.850069Z","shell.execute_reply":"2024-03-16T20:39:41.858101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-03-16T20:39:41.889158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking null values exist in dataframe\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking duplicated rows or records in dataframe\ndf.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are 403 duplicated rows in dataframe, so remove duplicates\ndf.drop_duplicates(inplace=True, keep=\"first\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EDA","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting target column\nfigure, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,5))\n\ncustom_colors = [\"#19b558\", \"#62bcde\"]\ndf[\"target\"].value_counts().plot(kind=\"pie\", autopct=\"%.1f%%\", colors = custom_colors, ax=ax1)\nfig = sns.countplot(x=df[\"target\"], palette=custom_colors, ax=ax2)\nfor bar in fig.containers:\n    fig.bar_label(bar)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\n\n\n<div style=\"background-color: #f0f0f0; padding: 20px; border-radius: 10px;\">\n    <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333333; text-align: justify;\">\n       From the above plot, we can see that 1(spam) category is very minimum as compared to 0(ham) category. It shows data is <b>imbalanced</b>.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"Now we are going to make 3 `new` columns:\n\n1. Number of Characters\n2. Number of Words\n3. Number of Sentences","metadata":{}},{"cell_type":"code","source":"# number of characters\ndf[\"num_characters\"] = df[\"text\"].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of words\ndf[\"num_words\"] = df[\"text\"].apply(lambda x: len(nltk.word_tokenize(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of sentences\ndf[\"num_sentences\"] = df[\"text\"].apply(lambda x: len(nltk.sent_tokenize(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"num_characters\",\"num_words\",\"num_sentences\"]].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ham emails/messages\ndf[df[\"target\"]==0][[\"num_characters\",\"num_words\",\"num_sentences\"]].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spam emails/messages\ndf[df[\"target\"]==1][[\"num_characters\",\"num_words\",\"num_sentences\"]].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting: number of characters in spam and ham emails\nplt.figure(figsize=(12,5))\nsns.histplot(df[df[\"target\"]==0][\"num_characters\"])\nsns.histplot(df[df[\"target\"]==1][\"num_characters\"], color=\"red\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting: number of words in spam and ham emails\nplt.figure(figsize=(12,5))\nsns.histplot(df[df[\"target\"]==0][\"num_words\"])\nsns.histplot(df[df[\"target\"]==1][\"num_words\"], color=\"red\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting: number of sentences in spam and ham emails\nplt.figure(figsize=(12,5))\nsns.histplot(df[df[\"target\"]==0][\"num_sentences\"])\nsns.histplot(df[df[\"target\"]==1][\"num_sentences\"], color=\"red\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting pairplot to see relationship between new columns\nsns.pairplot(df, hue=\"target\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation heatmap\nsns.heatmap(df.select_dtypes([\"int\"]).corr(), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\n\n\n<div style=\"background-color: #f0f0f0; padding: 20px; border-radius: 10px;\">\n    <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333333; text-align: justify;\">\n       New columns such as number of characters, words and sentences have strong relationship between them that shows there is <b>High Multicollinearity</b> between these columns. So, we will not use these columns or features in <b>Model Training</b> but will perform analysis through these features.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Here are the tasks performed in `text preprocessing`:\n\n- Lowercase\n- Tokenization\n- Removing Special Characters\n- Removing stop words and punctuation\n- Stemming","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport string\nfrom nltk.stem.porter import PorterStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_text(text):\n    # 01: transforming text into lower case\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    \n    # 02: getting alphnumeric content from text\n    y = []\n    for word in text:\n        if word.isalnum():\n            y.append(word)\n    \n    # 03: removing stop words and punction marks from text\n    text = y[:]\n    y.clear()\n    for word in text:\n        if word not in stopwords.words(\"english\") and word not in string.punctuation:\n            y.append(word)\n            \n    # 04: apply stemming \n    text = y[:]\n    y.clear()\n    for word in text:\n        y.append(PorterStemmer().stem(word))\n    \n    return \" \".join(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing the function\ntransform_text(\"ALi is goods goods how where boy's# ;$# ... >>(a)// !\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"transformed_text\"] = df[\"text\"].apply(transform_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performing analysis on ham and spam emails separately to see common and repeating words through Word Cloud\nfrom wordcloud import WordCloud\n\nwc = WordCloud(width=600, height=500, min_font_size=12, background_color=\"white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for ham emails/messages\nham_wc = wc.generate(df[df[\"target\"]==0][\"transformed_text\"].str.cat(sep=\" \"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(ham_wc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for spam emails/messages\nspam_wc = wc.generate(df[df[\"target\"]==1][\"transformed_text\"].str.cat(sep=\" \"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(spam_wc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ploting top repeated words\nfrom collections import Counter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top_words(target):\n    words = []\n    for msg in df[df[\"target\"] == target][\"transformed_text\"].tolist():\n        for word in msg.split():\n            words.append(word)\n            \n    sns.barplot(x=pd.DataFrame(Counter(words).most_common(30))[0], y=pd.DataFrame(Counter(words).most_common(30))[1])\n    plt.xticks(rotation=\"vertical\")\n    plt.xlabel(\"Words\")\n    plt.ylabel(\"Frequency\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_words(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_words(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Building","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using TF-IDF vectorizer\ntf_idf = TfidfVectorizer(max_features=3500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = tf_idf.fit_transform(df[\"transformed_text\"]).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df[\"target\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data into training and testing\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing algorithms\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\n\n\n<div style=\"background-color: #f0f0f0; padding: 20px; border-radius: 10px;\">\n    <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333333; text-align: justify;\">\n      I experimented with several methods, including <b>Naive Bayes and Gradient Boosting Classifier, Random Forest Classifier, Decision Tree Classifier, KNeighbors Classifier, SVC (Support Vector Classifier), Adaboost Classifier, Extra Trees Classifier, XGB Classifier, LightGBM Classifier, and so on. However, only the Naive Bayes method outperformed the rest. The code to test Naive Bayes algorithms is then written below. All algorithms are available for testing.\n\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"for model in [GaussianNB(), MultinomialNB(), BernoulliNB()]:\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    print(f\"{model}\")\n    print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\")\n    print(f\"Precision Score: {precision_score(y_test, y_pred)}\")\n    print(f\"Confusion Matrix : \\n{confusion_matrix(y_test, y_pred)}\")\n    print(\"\\n===================\\n\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note:\n\n\n<div style=\"background-color: #f0f0f0; padding: 20px; border-radius: 10px;\">\n    <p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333333; text-align: justify;\">\n        From <b>Naive Bayes</b> algorithms, only <b>MultinomialNB()</b> algorithm performs well. So we will use this model in production part.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# test selected model accuracy\nmodel = MultinomialNB()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(accuracy_score(y_test, y_pred))\nprint(precision_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pickle Files\n","metadata":{}},{"cell_type":"code","source":"# uncomment the following code for pickling files\n\n\n# import pickle\n# pickle.dump(tf_idf, open(\"vectorizer.pkl\", \"wb\"))\n# pickle.dump(model, open(\"model.pkl\", \"wb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"color:Blue; font-weight:900;\">the end</p>","metadata":{}}]}